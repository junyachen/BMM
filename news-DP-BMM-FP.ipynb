{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from InitKmeans import InitKmeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "     \n",
    "def pre_processData(newsgroups_train):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(newsgroups_train)):\n",
    "        newsgroups_train[i] = newsgroups_train[i].lower()\n",
    "        #newsgroups_train[i] = tokenizer.tokenize(newsgroups_train[i])\n",
    "    #newsgroups_train = [[token for token in doc if not token.isdigit()] for doc in newsgroups_train]\n",
    "    newsgroups_train = [doc.split(' ') for doc in newsgroups_train]\n",
    "    return newsgroups_train\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from InitKmeans import InitKmeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import sys  \n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    newsgroups_train = list()\n",
    "    news_labels = {}\n",
    "    docID_username = {}\n",
    "    \n",
    "    i = 0\n",
    "    file_path=\"D:/part_news2_biterm.txt\"\n",
    "    with open(file_path) as fp:\n",
    "        lines = fp.read().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                text = json.loads(line)[\"bitermText\"].strip()\n",
    "                label = json.loads(line)[\"clusterNo\"]\n",
    "                newsgroups_train.append(text)\n",
    "                news_labels[i] = label\n",
    "                i+=1\n",
    "    fp.close()\n",
    "    \n",
    "    corpus = pre_processData(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_biterm = {}\n",
    "for docID in range(len(corpus)):\n",
    "    words = corpus[docID]\n",
    "    duplicate_biterm = []\n",
    "    doc_biterm[docID] = {}\n",
    "    for biterm in words:\n",
    "        hash_biterm = set(biterm.split(','))\n",
    "        if hash_biterm not in duplicate_biterm:\n",
    "            duplicate_biterm.append(hash_biterm)\n",
    "            doc_biterm[docID][biterm] = 1\n",
    "        else:\n",
    "            if biterm in doc_biterm[docID].keys():\n",
    "                doc_biterm[docID][biterm] += 1\n",
    "            else:\n",
    "                doc_biterm[docID][biterm] = 1\n",
    "doc_words = {}\n",
    "i_biterm = 0\n",
    "for docID in range(len(corpus)):\n",
    "    doc_words[docID] = {}\n",
    "    words = corpus[docID]\n",
    "    for biterm in words:\n",
    "        for word in biterm.split(','):\n",
    "            if word not in doc_words[docID].keys():\n",
    "                doc_words[docID][word] = 0\n",
    "            doc_words[docID][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0  ,docID  0\n",
      "len(Topics)  0\n",
      "iter:  0  ,docID  1000\n",
      "len(Topics)  119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a519fe89ceca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mbiterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                         \u001b[0meach_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbiterm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meach_word\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_z\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                                 \u001b[0mn_z\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docID_assign_z = {}\n",
    "m_z = {}\n",
    "n_z = {}\n",
    "n_w = {}\n",
    "n_b = {}\n",
    "Topics = []\n",
    "V = set()\n",
    "D = set()\n",
    "alpha = 0.6\n",
    "beta = 0.02\n",
    "total_iter = 10\n",
    "for iter in range(total_iter):\n",
    "    for docID in range(len(corpus)):\n",
    "        if docID % 1000 == 0:\n",
    "            print(\"iter: \",iter,\" ,docID \",docID)\n",
    "            print(\"len(Topics) \",len(Topics))\n",
    "        words = corpus[docID]\n",
    "        D.discard(docID)\n",
    "        if docID in docID_assign_z.keys():\n",
    "            before_k = docID_assign_z[docID]\n",
    "            m_z[before_k].discard(docID)\n",
    "            for biterm in words:\n",
    "                for word in biterm.split(','):\n",
    "                    n_z[before_k][word] -= 1\n",
    "                    n_w[before_k] -=1\n",
    "        else:\n",
    "            before_k = -1\n",
    "        if len(D) == 0 and len(V) == 0:\n",
    "            choose_k = 0\n",
    "            D.add(docID)\n",
    "            docID_assign_z[docID] = choose_k\n",
    "            if choose_k not in m_z.keys():\n",
    "                m_z[choose_k] = set()\n",
    "            m_z[choose_k].add(docID)\n",
    "            for biterm in words:\n",
    "                for word in biterm.split(','):\n",
    "                    if choose_k not in n_w.keys():\n",
    "                        n_w[choose_k] = 0\n",
    "                    if choose_k not in n_z.keys():\n",
    "                        n_z[choose_k] = {}\n",
    "                    if word not in n_z[choose_k].keys():\n",
    "                        n_z[choose_k][word] = 0\n",
    "                    n_z[choose_k][word] += 1\n",
    "                    n_w[choose_k] += 1\n",
    "                    V.add(word)\n",
    "            if choose_k not in Topics:\n",
    "                Topics.append(choose_k)\n",
    "        else:\n",
    "            log_pro = []\n",
    "            for k in Topics:\n",
    "                pro_k = len(m_z[k])\n",
    "                if pro_k != 0:\n",
    "                    i = 0\n",
    "                    for biterm in words:\n",
    "                        each_word = biterm.strip().split(',')\n",
    "                        for word in each_word:\n",
    "                            if word not in n_z[k].keys():\n",
    "                                n_z[k][word] = 0\n",
    "                        for j in range(doc_biterm[docID][biterm]):\n",
    "                            pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                            i += 1\n",
    "\n",
    "                if pro_k == 0:\n",
    "                    pro_k = sys.float_info.min\n",
    "                log_pro.append(pro_k)\n",
    "\n",
    "            pro_new_k = alpha*(len(D))\n",
    "            i = 0\n",
    "            for biterm in words:\n",
    "                for j in range(doc_biterm[docID][biterm]):\n",
    "                    pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                    i += 1\n",
    "\n",
    "            if pro_new_k == 0:\n",
    "                pro_new_k = sys.float_info.min        \n",
    "            log_pro.append(pro_new_k)\n",
    "\n",
    "            sum_pro=sum(log_pro)\n",
    "\n",
    "            normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "            select_k = None\n",
    "            if iter == (total_iter - 1):\n",
    "                select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "            else:\n",
    "                select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "            if select_k == len(Topics):\n",
    "                choose_k = np.max(Topics) + 1\n",
    "            else:\n",
    "                choose_k = Topics[select_k]\n",
    "\n",
    "            D.add(docID)\n",
    "            docID_assign_z[docID] = choose_k\n",
    "            if choose_k not in m_z.keys():\n",
    "                m_z[choose_k] = set()\n",
    "            m_z[choose_k].add(docID)\n",
    "            for biterm in words:\n",
    "                for word in biterm.split(','):\n",
    "                    if choose_k not in n_w.keys():\n",
    "                        n_w[choose_k] = 0\n",
    "                    if choose_k not in n_z.keys():\n",
    "                        n_z[choose_k] = {}\n",
    "                    if word not in n_z[choose_k].keys():\n",
    "                        n_z[choose_k][word] = 0\n",
    "                    n_z[choose_k][word] += 1\n",
    "                    n_w[choose_k] += 1\n",
    "                    V.add(word)\n",
    "            if choose_k not in Topics:\n",
    "                Topics.append(choose_k)\n",
    "\n",
    "        count_k = []\n",
    "        for k in Topics:\n",
    "            if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                m_z.pop(k, None)\n",
    "                n_z.pop(k, None)\n",
    "                n_w.pop(k, None)\n",
    "                count_k.append(k)\n",
    "        for k in count_k:\n",
    "            Topics.remove(k)   \n",
    "\n",
    "    from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "    nmi_sample = []\n",
    "    nmi_result = []\n",
    "    for key, value in news_labels.items():\n",
    "            nmi_sample.append(value)\n",
    "            nmi_result.append(docID_assign_z[key])\n",
    "    print(\"alpha \", alpha, \"Topics \",len(Topics), \" iter \",iter,\" \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    alpha_NMI[alpha] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "    alpha_topics[alpha] = len(Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0  end  694 topics:  120 truth:  132  NMI:  0.8882984363075321\n",
      "start  0  end  1388 topics:  131 truth:  143  NMI:  0.8655685741007803\n",
      "start  695  end  2082 topics:  134 truth:  146  NMI:  0.8601966096751001\n",
      "start  1389  end  2776 topics:  125 truth:  142  NMI:  0.8653117694146474\n",
      "start  2083  end  3470 topics:  131 truth:  143  NMI:  0.8691099008525507\n",
      "start  2777  end  4164 topics:  130 truth:  140  NMI:  0.870470086290841\n",
      "start  3471  end  4858 topics:  131 truth:  142  NMI:  0.8850242362688984\n",
      "start  4165  end  5552 topics:  126 truth:  137  NMI:  0.8614462503512703\n",
      "start  4859  end  6246 topics:  134 truth:  139  NMI:  0.8526164464109245\n",
      "start  5553  end  6940 topics:  137 truth:  137  NMI:  0.8724485266855552\n",
      "start  6247  end  7634 topics:  130 truth:  139  NMI:  0.8775498841669781\n",
      "start  6941  end  8328 topics:  131 truth:  144  NMI:  0.8580625490011807\n",
      "start  7635  end  9022 topics:  141 truth:  147  NMI:  0.8604648478317101\n",
      "start  8329  end  9716 topics:  132 truth:  147  NMI:  0.8698139510080556\n",
      "start  9023  end  10410 topics:  122 truth:  139  NMI:  0.8714141829613143\n",
      "start  9717  end  11109 topics:  135 truth:  138  NMI:  0.8574559445509717\n"
     ]
    }
   ],
   "source": [
    "total_batch = 16\n",
    "NMI_batch = {}\n",
    "start = 0\n",
    "end = 0\n",
    "topic_batch = {}\n",
    "for batch_i in range(1,17):\n",
    "    if batch_i == 16:\n",
    "        end = int(len(corpus))\n",
    "    else:\n",
    "        end = int(len(corpus) / total_batch) * batch_i\n",
    "    \n",
    "    docID_assign_z = {}\n",
    "    m_z = {}\n",
    "    n_z = {}\n",
    "    n_w = {}\n",
    "    n_b = {}\n",
    "    Topics = []\n",
    "    V = set()\n",
    "    D = set()\n",
    "    alpha = 0.8\n",
    "    beta = 0.02\n",
    "    NMI_batch[batch_i] = 0\n",
    "    topic_batch[batch_i] = 0\n",
    "    \n",
    "    total_iter = 10\n",
    "    for iter in range(total_iter):\n",
    "        for docID in range(start,end):\n",
    "            words = corpus[docID]\n",
    "            D.discard(docID)\n",
    "            if docID in docID_assign_z.keys():\n",
    "                before_k = docID_assign_z[docID]\n",
    "                m_z[before_k].discard(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        n_z[before_k][word] -= 1\n",
    "                        n_w[before_k] -=1\n",
    "            else:\n",
    "                before_k = -1\n",
    "            if len(D) == 0 and len(V) == 0:\n",
    "                choose_k = 0\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "            else:\n",
    "                log_pro = []\n",
    "                for k in Topics:\n",
    "                    pro_k = len(m_z[k])\n",
    "                    if pro_k != 0:\n",
    "                        i = 0\n",
    "                        for biterm in words:\n",
    "                            each_word = biterm.strip().split(',')\n",
    "                            for word in each_word:\n",
    "                                if word not in n_z[k].keys():\n",
    "                                    n_z[k][word] = 0\n",
    "                            for j in range(doc_biterm[docID][biterm]):\n",
    "                                pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                i += 1\n",
    "\n",
    "                    if pro_k == 0:\n",
    "                        pro_k = sys.float_info.min\n",
    "                    log_pro.append(pro_k)\n",
    "\n",
    "                pro_new_k = alpha*(len(D))\n",
    "                i = 0\n",
    "                for biterm in words:\n",
    "                    for j in range(doc_biterm[docID][biterm]):\n",
    "                        pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                        i += 1\n",
    "\n",
    "                if pro_new_k == 0:\n",
    "                    pro_new_k = sys.float_info.min        \n",
    "                log_pro.append(pro_new_k)\n",
    "\n",
    "                sum_pro=sum(log_pro)\n",
    "\n",
    "                normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                select_k = None\n",
    "                if iter == (total_iter - 1):\n",
    "                    select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                else:\n",
    "                    select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                if select_k == len(Topics):\n",
    "                    choose_k = np.max(Topics) + 1\n",
    "                else:\n",
    "                    choose_k = Topics[select_k]\n",
    "\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "\n",
    "            count_k = []\n",
    "            for k in Topics:\n",
    "                if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                    m_z.pop(k, None)\n",
    "                    n_z.pop(k, None)\n",
    "                    n_w.pop(k, None)\n",
    "                    count_k.append(k)\n",
    "            for k in count_k:\n",
    "                Topics.remove(k)   \n",
    "        if iter == 9:\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            NMI_batch[batch_i] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "            topic_batch[batch_i] = len(Topics)\n",
    "            print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    if batch_i != 1:\n",
    "         start = int(len(corpus) / total_batch) * (batch_i -1) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "[0.8791632071654407, 0.8689766289541706, 0.8520647369847433, 0.8672141455434031, 0.8669673186133002, 0.8625201831434595, 0.8767862446229376, 0.8581853527328299, 0.8442325374746548, 0.8599915553662428, 0.8743825671633735, 0.8612640060536784, 0.8519235493142886, 0.8691149373848646, 0.86462425414101, 0.8621060128272594]\n",
      "\n",
      "0.8637198273428535\n"
     ]
    }
   ],
   "source": [
    "common_keys = []\n",
    "\n",
    "item_NMI = []\n",
    "for key,value in sorted(NMI_batch.items(), key=lambda x: x[0]):\n",
    "    common_keys.append(key)\n",
    "    item_NMI.append(value)\n",
    "print(common_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "print(item_NMI)\n",
    "print()\n",
    "\n",
    "print(np.mean(item_NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "[0.8730133849433831, 0.8684743694996326, 0.8804032259273199, 0.8758633536436381, 0.8656884137743128, 0.8843568887366188, 0.8758842887943914, 0.8596648235205762, 0.8664921845605948, 0.8776708653357372, 0.875552963171944, 0.8499807697201824, 0.8813713693485388, 0.871907891615146, 0.8641084076115443, 0.8646719585908315]\n",
      "\n",
      "0.8709440724246496\n"
     ]
    }
   ],
   "source": [
    "common_keys = []\n",
    "\n",
    "item_NMI = []\n",
    "for key,value in sorted(NMI_batch.items(), key=lambda x: x[0]):\n",
    "    common_keys.append(key)\n",
    "    item_NMI.append(value)\n",
    "print(common_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "print(item_NMI)\n",
    "print()\n",
    "\n",
    "print(np.mean(item_NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8709440724246496\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(item_NMI))\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.8882984363075321, 2: 0.8655685741007803, 3: 0.8601966096751001, 4: 0.8653117694146474, 5: 0.8691099008525507, 6: 0.870470086290841, 7: 0.8850242362688984, 8: 0.8614462503512703, 9: 0.8526164464109245, 10: 0.8724485266855552, 11: 0.8775498841669781, 12: 0.8580625490011807, 13: 0.8604648478317101, 14: 0.8698139510080556, 15: 0.8714141829613143, 16: 0.8574559445509717}\n",
      "0.02\n",
      "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "dict_values([0.8882984363075321, 0.8655685741007803, 0.8601966096751001, 0.8653117694146474, 0.8691099008525507, 0.870470086290841, 0.8850242362688984, 0.8614462503512703, 0.8526164464109245, 0.8724485266855552, 0.8775498841669781, 0.8580625490011807, 0.8604648478317101, 0.8698139510080556, 0.8714141829613143, 0.8574559445509717])\n"
     ]
    }
   ],
   "source": [
    "print(NMI_batch)\n",
    "print(beta)\n",
    "print(NMI_batch.keys())\n",
    "print(NMI_batch.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0  end  500 topics:  121 truth:  119  NMI:  0.8816253485292893\n",
      "start  500  end  1000 topics:  120 truth:  118  NMI:  0.8896958498673855\n",
      "start  1000  end  1500 topics:  124 truth:  120  NMI:  0.887547430347356\n",
      "start  1500  end  2000 topics:  114 truth:  120  NMI:  0.8798078001390809\n",
      "start  2000  end  2500 topics:  115 truth:  117  NMI:  0.8758785113006561\n",
      "start  2500  end  3000 topics:  129 truth:  125  NMI:  0.8878789420209223\n",
      "start  3000  end  3500 topics:  121 truth:  119  NMI:  0.8858948989230734\n",
      "start  3500  end  4000 topics:  126 truth:  120  NMI:  0.8956560454451744\n",
      "start  4000  end  4500 topics:  118 truth:  120  NMI:  0.8906988343883995\n",
      "start  4500  end  5000 topics:  118 truth:  118  NMI:  0.8759524080214447\n",
      "start  5000  end  5500 topics:  114 truth:  106  NMI:  0.8669809275843139\n",
      "start  5500  end  6000 topics:  121 truth:  113  NMI:  0.8724444083741711\n",
      "start  6000  end  6500 topics:  116 truth:  107  NMI:  0.8799065794215711\n",
      "start  6500  end  7000 topics:  123 truth:  120  NMI:  0.8989565220960359\n",
      "start  7000  end  7500 topics:  116 truth:  116  NMI:  0.8896116490251628\n",
      "start  7500  end  8000 topics:  133 truth:  122  NMI:  0.8756882119257414\n",
      "start  8000  end  8500 topics:  125 truth:  116  NMI:  0.8769124306982721\n",
      "start  8500  end  9000 topics:  123 truth:  125  NMI:  0.8938102522988703\n",
      "start  9000  end  9500 topics:  108 truth:  119  NMI:  0.8854914948128427\n",
      "start  9500  end  10000 topics:  112 truth:  113  NMI:  0.8847628129754427\n",
      "start  10000  end  10500 topics:  123 truth:  118  NMI:  0.8684046082432879\n",
      "start  10500  end  11000 topics:  127 truth:  124  NMI:  0.8840466577094394\n",
      "start  11000  end  11109 topics:  61 truth:  74  NMI:  0.929462199547604\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22])\n",
      "dict_values([0.8816253485292893, 0.8896958498673855, 0.887547430347356, 0.8798078001390809, 0.8758785113006561, 0.8878789420209223, 0.8858948989230734, 0.8956560454451744, 0.8906988343883995, 0.8759524080214447, 0.8669809275843139, 0.8724444083741711, 0.8799065794215711, 0.8989565220960359, 0.8896116490251628, 0.8756882119257414, 0.8769124306982721, 0.8938102522988703, 0.8854914948128427, 0.8847628129754427, 0.8684046082432879, 0.8840466577094394, 0.929462199547604])\n"
     ]
    }
   ],
   "source": [
    "nmi_batch = {}\n",
    "topic_batch = {}\n",
    "start = 0\n",
    "end = 0\n",
    "total_batch = None\n",
    "if len(corpus) % 500 == 0:\n",
    "    total_batch = int(len(corpus) / 500)\n",
    "else:\n",
    "    total_batch = int(len(corpus) / 500) +1\n",
    "\n",
    "for batch in range(total_batch):\n",
    "    docID_assign_z = {}\n",
    "    m_z = {}\n",
    "    n_z = {}\n",
    "    n_w = {}\n",
    "    n_b = {}\n",
    "    Topics = []\n",
    "    V = set()\n",
    "    D = set()\n",
    "    alpha = 0.6\n",
    "    beta = 0.002\n",
    "    end = 500 * (batch + 1)\n",
    "    if end > len(corpus):\n",
    "        end = len(corpus)\n",
    "    nmi_batch[batch] = 0\n",
    "    topic_batch[batch] = 0\n",
    "    \n",
    "    total_iter = 10\n",
    "    for iter in range(total_iter):\n",
    "        for docID in range(start,end):\n",
    "            words = corpus[docID]\n",
    "            D.discard(docID)\n",
    "            if docID in docID_assign_z.keys():\n",
    "                before_k = docID_assign_z[docID]\n",
    "                m_z[before_k].discard(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        n_z[before_k][word] -= 1\n",
    "                        n_w[before_k] -=1\n",
    "            else:\n",
    "                before_k = -1\n",
    "            if len(D) == 0 and len(V) == 0:\n",
    "                choose_k = 0\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "            else:\n",
    "                log_pro = []\n",
    "                for k in Topics:\n",
    "                    pro_k = len(m_z[k])\n",
    "                    if pro_k != 0:\n",
    "                        i = 0\n",
    "                        for biterm in words:\n",
    "                            each_word = biterm.strip().split(',')\n",
    "                            for word in each_word:\n",
    "                                if word not in n_z[k].keys():\n",
    "                                    n_z[k][word] = 0\n",
    "                            for j in range(doc_biterm[docID][biterm]):\n",
    "                                pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                i += 1\n",
    "\n",
    "                    if pro_k == 0:\n",
    "                        pro_k = sys.float_info.min\n",
    "                    log_pro.append(pro_k)\n",
    "\n",
    "                pro_new_k = alpha*(len(D))\n",
    "                i = 0\n",
    "                for biterm in words:\n",
    "                    for j in range(doc_biterm[docID][biterm]):\n",
    "                        pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                        i += 1\n",
    "\n",
    "                if pro_new_k == 0:\n",
    "                    pro_new_k = sys.float_info.min        \n",
    "                log_pro.append(pro_new_k)\n",
    "\n",
    "                sum_pro=sum(log_pro)\n",
    "\n",
    "                normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                select_k = None\n",
    "                if iter == (total_iter - 1):\n",
    "                    select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                else:\n",
    "                    select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                if select_k == len(Topics):\n",
    "                    choose_k = np.max(Topics) + 1\n",
    "                else:\n",
    "                    choose_k = Topics[select_k]\n",
    "\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "\n",
    "            count_k = []\n",
    "            for k in Topics:\n",
    "                if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                    m_z.pop(k, None)\n",
    "                    n_z.pop(k, None)\n",
    "                    n_w.pop(k, None)\n",
    "                    count_k.append(k)\n",
    "            for k in count_k:\n",
    "                Topics.remove(k)   \n",
    "        if iter == 9:\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            nmi_batch[batch] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "            topic_batch[batch] = len(Topics)\n",
    "            print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    start = end \n",
    " \n",
    "print(nmi_batch.keys())\n",
    "print(nmi_batch.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0  end  694 topics:  138 truth:  132  NMI:  0.8909870709330914\n",
      "start  0  end  1388 topics:  155 truth:  143  NMI:  0.8651699783582908\n",
      "start  695  end  2082 topics:  164 truth:  146  NMI:  0.8672140012352513\n",
      "start  1389  end  2776 topics:  142 truth:  142  NMI:  0.8680859675421644\n",
      "start  2083  end  3470 topics:  153 truth:  143  NMI:  0.8717006857362761\n",
      "start  2777  end  4164 topics:  153 truth:  140  NMI:  0.8744622753269023\n",
      "start  3471  end  4858 topics:  154 truth:  142  NMI:  0.8800267550934175\n",
      "start  4165  end  5552 topics:  153 truth:  137  NMI:  0.8678603290385756\n",
      "start  4859  end  6246 topics:  150 truth:  139  NMI:  0.8516026112257538\n",
      "start  5553  end  6940 topics:  156 truth:  137  NMI:  0.8706066920056388\n",
      "start  6247  end  7634 topics:  147 truth:  139  NMI:  0.8771950844789724\n",
      "start  6941  end  8328 topics:  161 truth:  144  NMI:  0.8670647824143706\n",
      "start  7635  end  9022 topics:  167 truth:  147  NMI:  0.8688979969642097\n",
      "start  8329  end  9716 topics:  151 truth:  147  NMI:  0.8707276568667933\n",
      "start  9023  end  10410 topics:  151 truth:  139  NMI:  0.8661519878300761\n",
      "start  9717  end  11109 topics:  158 truth:  138  NMI:  0.8629784090033633\n"
     ]
    }
   ],
   "source": [
    "total_batch = 16\n",
    "NMI_batch = {}\n",
    "start = 0\n",
    "end = 0\n",
    "topic_batch = {}\n",
    "for batch_i in range(1,17):\n",
    "    if batch_i == 16:\n",
    "        end = int(len(corpus))\n",
    "    else:\n",
    "        end = int(len(corpus) / total_batch) * batch_i\n",
    "    \n",
    "    docID_assign_z = {}\n",
    "    m_z = {}\n",
    "    n_z = {}\n",
    "    n_w = {}\n",
    "    n_b = {}\n",
    "    Topics = []\n",
    "    V = set()\n",
    "    D = set()\n",
    "    alpha = 0.6\n",
    "    beta = 0.006\n",
    "    NMI_batch[batch_i] = 0\n",
    "    topic_batch[batch_i] = 0\n",
    "    \n",
    "    total_iter = 10\n",
    "    for iter in range(total_iter):\n",
    "        for docID in range(start,end):\n",
    "            words = corpus[docID]\n",
    "            D.discard(docID)\n",
    "            if docID in docID_assign_z.keys():\n",
    "                before_k = docID_assign_z[docID]\n",
    "                m_z[before_k].discard(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        n_z[before_k][word] -= 1\n",
    "                        n_w[before_k] -=1\n",
    "            else:\n",
    "                before_k = -1\n",
    "            if len(D) == 0 and len(V) == 0:\n",
    "                choose_k = 0\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "            else:\n",
    "                log_pro = []\n",
    "                for k in Topics:\n",
    "                    pro_k = len(m_z[k])\n",
    "                    if pro_k != 0:\n",
    "                        i = 0\n",
    "                        for biterm in words:\n",
    "                            each_word = biterm.strip().split(',')\n",
    "                            for word in each_word:\n",
    "                                if word not in n_z[k].keys():\n",
    "                                    n_z[k][word] = 0\n",
    "                            for j in range(doc_biterm[docID][biterm]):\n",
    "                                pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                i += 1\n",
    "\n",
    "                    if pro_k == 0:\n",
    "                        pro_k = sys.float_info.min\n",
    "                    log_pro.append(pro_k)\n",
    "\n",
    "                pro_new_k = alpha*(len(D))\n",
    "                i = 0\n",
    "                for biterm in words:\n",
    "                    for j in range(doc_biterm[docID][biterm]):\n",
    "                        pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                        i += 1\n",
    "\n",
    "                if pro_new_k == 0:\n",
    "                    pro_new_k = sys.float_info.min        \n",
    "                log_pro.append(pro_new_k)\n",
    "\n",
    "                sum_pro=sum(log_pro)\n",
    "\n",
    "                normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                select_k = None\n",
    "                if iter == (total_iter - 1):\n",
    "                    select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                else:\n",
    "                    select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                if select_k == len(Topics):\n",
    "                    choose_k = np.max(Topics) + 1\n",
    "                else:\n",
    "                    choose_k = Topics[select_k]\n",
    "\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "\n",
    "            count_k = []\n",
    "            for k in Topics:\n",
    "                if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                    m_z.pop(k, None)\n",
    "                    n_z.pop(k, None)\n",
    "                    n_w.pop(k, None)\n",
    "                    count_k.append(k)\n",
    "            for k in count_k:\n",
    "                Topics.remove(k)   \n",
    "        if iter == 9:\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            NMI_batch[batch_i] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "            topic_batch[batch_i] = len(Topics)\n",
    "            print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    if batch_i != 1:\n",
    "        start = int(len(corpus) / total_batch) * (batch_i -1) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "[0.8909870709330914, 0.8651699783582908, 0.8672140012352513, 0.8680859675421644, 0.8717006857362761, 0.8744622753269023, 0.8800267550934175, 0.8678603290385756, 0.8516026112257538, 0.8706066920056388, 0.8771950844789724, 0.8670647824143706, 0.8688979969642097, 0.8707276568667933, 0.8661519878300761, 0.8629784090033633]\n",
      "0.8700457677533218\n"
     ]
    }
   ],
   "source": [
    "common_keys = []\n",
    "\n",
    "item_NMI = []\n",
    "for key,value in sorted(NMI_batch.items(), key=lambda x: x[0]):\n",
    "    common_keys.append(key)\n",
    "    item_NMI.append(value)\n",
    "print(common_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "print(item_NMI)\n",
    "print(np.mean(item_NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0  end  694 topics:  113 truth:  132  NMI:  0.8836736821351552\n",
      "start  0  end  1388 topics:  127 truth:  143  NMI:  0.8678265020507938\n",
      "start  695  end  2082 topics:  134 truth:  146  NMI:  0.864495267593281\n",
      "start  1389  end  2776 topics:  123 truth:  142  NMI:  0.8706820058256952\n",
      "start  2083  end  3470 topics:  127 truth:  143  NMI:  0.8631459468372141\n",
      "start  2777  end  4164 topics:  124 truth:  140  NMI:  0.8708992967204708\n",
      "start  3471  end  4858 topics:  123 truth:  142  NMI:  0.8765553290209164\n",
      "start  4165  end  5552 topics:  121 truth:  137  NMI:  0.8551546457480605\n",
      "start  4859  end  6246 topics:  126 truth:  139  NMI:  0.846603071665342\n",
      "start  5553  end  6940 topics:  131 truth:  137  NMI:  0.8639294618581334\n",
      "start  6247  end  7634 topics:  122 truth:  139  NMI:  0.8799141431225486\n",
      "start  6941  end  8328 topics:  134 truth:  144  NMI:  0.8568565127760674\n",
      "start  7635  end  9022 topics:  132 truth:  147  NMI:  0.8540550286342588\n",
      "start  8329  end  9716 topics:  121 truth:  147  NMI:  0.8645514921543302\n",
      "start  9023  end  10410 topics:  125 truth:  139  NMI:  0.8606425327935067\n",
      "start  9717  end  11109 topics:  127 truth:  138  NMI:  0.8625456833956983\n"
     ]
    }
   ],
   "source": [
    "total_batch = 16\n",
    "NMI_batch = {}\n",
    "start = 0\n",
    "end = 0\n",
    "topic_batch = {}\n",
    "for batch_i in range(1,17):\n",
    "    if batch_i == 16:\n",
    "        end = int(len(corpus))\n",
    "    else:\n",
    "        end = int(len(corpus) / total_batch) * batch_i\n",
    "    \n",
    "    docID_assign_z = {}\n",
    "    m_z = {}\n",
    "    n_z = {}\n",
    "    n_w = {}\n",
    "    n_b = {}\n",
    "    Topics = []\n",
    "    V = set()\n",
    "    D = set()\n",
    "    alpha = 0.6\n",
    "    beta = 0.02\n",
    "    NMI_batch[batch_i] = 0\n",
    "    topic_batch[batch_i] = 0\n",
    "    \n",
    "    total_iter = 10\n",
    "    for iter in range(total_iter):\n",
    "        for docID in range(start,end):\n",
    "            words = corpus[docID]\n",
    "            D.discard(docID)\n",
    "            if docID in docID_assign_z.keys():\n",
    "                before_k = docID_assign_z[docID]\n",
    "                m_z[before_k].discard(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        n_z[before_k][word] -= 1\n",
    "                        n_w[before_k] -=1\n",
    "            else:\n",
    "                before_k = -1\n",
    "            if len(D) == 0 and len(V) == 0:\n",
    "                choose_k = 0\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "            else:\n",
    "                log_pro = []\n",
    "                for k in Topics:\n",
    "                    pro_k = len(m_z[k])\n",
    "                    if pro_k != 0:\n",
    "                        i = 0\n",
    "                        for biterm in words:\n",
    "                            each_word = biterm.strip().split(',')\n",
    "                            for word in each_word:\n",
    "                                if word not in n_z[k].keys():\n",
    "                                    n_z[k][word] = 0\n",
    "                            for j in range(doc_biterm[docID][biterm]):\n",
    "                                pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                i += 1\n",
    "\n",
    "                    if pro_k == 0:\n",
    "                        pro_k = sys.float_info.min\n",
    "                    log_pro.append(pro_k)\n",
    "\n",
    "                pro_new_k = alpha*(len(D))\n",
    "                i = 0\n",
    "                for biterm in words:\n",
    "                    for j in range(doc_biterm[docID][biterm]):\n",
    "                        pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                        i += 1\n",
    "\n",
    "                if pro_new_k == 0:\n",
    "                    pro_new_k = sys.float_info.min        \n",
    "                log_pro.append(pro_new_k)\n",
    "\n",
    "                sum_pro=sum(log_pro)\n",
    "\n",
    "                normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                select_k = None\n",
    "                if iter == (total_iter - 1):\n",
    "                    select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                else:\n",
    "                    select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                if select_k == len(Topics):\n",
    "                    choose_k = np.max(Topics) + 1\n",
    "                else:\n",
    "                    choose_k = Topics[select_k]\n",
    "\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "\n",
    "            count_k = []\n",
    "            for k in Topics:\n",
    "                if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                    m_z.pop(k, None)\n",
    "                    n_z.pop(k, None)\n",
    "                    n_w.pop(k, None)\n",
    "                    count_k.append(k)\n",
    "            for k in count_k:\n",
    "                Topics.remove(k)   \n",
    "        if iter == 9:\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            NMI_batch[batch_i] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "            topic_batch[batch_i] = len(Topics)\n",
    "            print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    if batch_i != 1:\n",
    "        start = int(len(corpus) / total_batch) * (batch_i -1) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "[0.8836736821351552, 0.8678265020507938, 0.864495267593281, 0.8706820058256952, 0.8631459468372141, 0.8708992967204708, 0.8765553290209164, 0.8551546457480605, 0.846603071665342, 0.8639294618581334, 0.8799141431225486, 0.8568565127760674, 0.8540550286342588, 0.8645514921543302, 0.8606425327935067, 0.8625456833956983]\n",
      "0.8650956626457169\n"
     ]
    }
   ],
   "source": [
    "common_keys = []\n",
    "\n",
    "item_NMI = []\n",
    "for key,value in sorted(NMI_batch.items(), key=lambda x: x[0]):\n",
    "    common_keys.append(key)\n",
    "    item_NMI.append(value)\n",
    "print(common_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "print(item_NMI)\n",
    "print(np.mean(item_NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0  end  694 topics:  112 truth:  132  NMI:  0.8762459039342183\n",
      "[27.288816929872613]\n",
      "start  0  end  1388 topics:  128 truth:  143  NMI:  0.860589449245971\n",
      "[27.288816929872613, 59.55673653274858]\n",
      "start  695  end  2082 topics:  127 truth:  146  NMI:  0.8657264330184065\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947]\n",
      "start  1389  end  2776 topics:  121 truth:  142  NMI:  0.860015590313485\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846]\n",
      "start  2083  end  3470 topics:  128 truth:  143  NMI:  0.8711558438895886\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081]\n",
      "start  2777  end  4164 topics:  127 truth:  140  NMI:  0.8728882689844335\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956]\n",
      "start  3471  end  4858 topics:  126 truth:  142  NMI:  0.8746967476654431\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905]\n",
      "start  4165  end  5552 topics:  121 truth:  137  NMI:  0.854585265795717\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965]\n",
      "start  4859  end  6246 topics:  120 truth:  139  NMI:  0.848169299929925\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162]\n",
      "start  5553  end  6940 topics:  130 truth:  137  NMI:  0.8698905910056118\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614]\n",
      "start  6247  end  7634 topics:  128 truth:  139  NMI:  0.8731410703068065\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144]\n",
      "start  6941  end  8328 topics:  136 truth:  144  NMI:  0.8574979891152124\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144, 57.82417238033747]\n",
      "start  7635  end  9022 topics:  140 truth:  147  NMI:  0.8628237642272402\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144, 57.82417238033747, 59.22013599833417]\n",
      "start  8329  end  9716 topics:  134 truth:  147  NMI:  0.8694029898159286\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144, 57.82417238033747, 59.22013599833417, 58.4134545265523]\n",
      "start  9023  end  10410 topics:  122 truth:  139  NMI:  0.8660484168048693\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144, 57.82417238033747, 59.22013599833417, 58.4134545265523, 54.17401338710124]\n",
      "start  9717  end  11109 topics:  130 truth:  138  NMI:  0.8590876905290958\n",
      "[27.288816929872613, 59.55673653274858, 64.34812713088947, 60.149568120706846, 58.97903506745081, 56.51260873601956, 57.602352252571905, 53.000971717141965, 52.942498022162, 55.09169386162614, 56.525054467540144, 57.82417238033747, 59.22013599833417, 58.4134545265523, 54.17401338710124, 61.58635948306096]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "total_batch = 16\n",
    "NMI_batch = {}\n",
    "start = 0\n",
    "end = 0\n",
    "topic_batch = {}\n",
    "time_iteration_list = []\n",
    "for batch_i in range(1,17):\n",
    "    if batch_i == 16:\n",
    "        end = int(len(corpus))\n",
    "    else:\n",
    "        end = int(len(corpus) / total_batch) * batch_i\n",
    "    \n",
    "    docID_assign_z = {}\n",
    "    m_z = {}\n",
    "    n_z = {}\n",
    "    n_w = {}\n",
    "    n_b = {}\n",
    "    Topics = []\n",
    "    V = set()\n",
    "    D = set()\n",
    "    alpha = 0.6\n",
    "    beta = 0.02\n",
    "    NMI_batch[batch_i] = 0\n",
    "    topic_batch[batch_i] = 0\n",
    "    \n",
    "    time_start = timeit.default_timer()\n",
    "    total_iter = 10\n",
    "    for iter in range(total_iter):\n",
    "        for docID in range(start,end):\n",
    "            words = corpus[docID]\n",
    "            D.discard(docID)\n",
    "            if docID in docID_assign_z.keys():\n",
    "                before_k = docID_assign_z[docID]\n",
    "                m_z[before_k].discard(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        n_z[before_k][word] -= 1\n",
    "                        n_w[before_k] -=1\n",
    "            else:\n",
    "                before_k = -1\n",
    "            if len(D) == 0 and len(V) == 0:\n",
    "                choose_k = 0\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "            else:\n",
    "                log_pro = []\n",
    "                for k in Topics:\n",
    "                    pro_k = len(m_z[k])\n",
    "                    if pro_k != 0:\n",
    "                        i = 0\n",
    "                        for biterm in words:\n",
    "                            each_word = biterm.strip().split(',')\n",
    "                            for word in each_word:\n",
    "                                if word not in n_z[k].keys():\n",
    "                                    n_z[k][word] = 0\n",
    "                            for j in range(doc_biterm[docID][biterm]):\n",
    "                                pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                i += 1\n",
    "\n",
    "                    if pro_k == 0:\n",
    "                        pro_k = sys.float_info.min\n",
    "                    log_pro.append(pro_k)\n",
    "\n",
    "                pro_new_k = alpha*(len(D))\n",
    "                i = 0\n",
    "                for biterm in words:\n",
    "                    for j in range(doc_biterm[docID][biterm]):\n",
    "                        pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                        i += 1\n",
    "\n",
    "                if pro_new_k == 0:\n",
    "                    pro_new_k = sys.float_info.min        \n",
    "                log_pro.append(pro_new_k)\n",
    "\n",
    "                sum_pro=sum(log_pro)\n",
    "\n",
    "                normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                select_k = None\n",
    "                if iter == (total_iter - 1):\n",
    "                    select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                else:\n",
    "                    select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                if select_k == len(Topics):\n",
    "                    choose_k = np.max(Topics) + 1\n",
    "                else:\n",
    "                    choose_k = Topics[select_k]\n",
    "\n",
    "                D.add(docID)\n",
    "                docID_assign_z[docID] = choose_k\n",
    "                if choose_k not in m_z.keys():\n",
    "                    m_z[choose_k] = set()\n",
    "                m_z[choose_k].add(docID)\n",
    "                for biterm in words:\n",
    "                    for word in biterm.split(','):\n",
    "                        if choose_k not in n_w.keys():\n",
    "                            n_w[choose_k] = 0\n",
    "                        if choose_k not in n_z.keys():\n",
    "                            n_z[choose_k] = {}\n",
    "                        if word not in n_z[choose_k].keys():\n",
    "                            n_z[choose_k][word] = 0\n",
    "                        n_z[choose_k][word] += 1\n",
    "                        n_w[choose_k] += 1\n",
    "                        V.add(word)\n",
    "                if choose_k not in Topics:\n",
    "                    Topics.append(choose_k)\n",
    "\n",
    "            count_k = []\n",
    "            for k in Topics:\n",
    "                if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                    m_z.pop(k, None)\n",
    "                    n_z.pop(k, None)\n",
    "                    n_w.pop(k, None)\n",
    "                    count_k.append(k)\n",
    "            for k in count_k:\n",
    "                Topics.remove(k)   \n",
    "        if iter == 9:\n",
    "            from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "            nmi_sample = []\n",
    "            nmi_result = []\n",
    "            for key, value in news_labels.items():\n",
    "                if key < end and key >= start:\n",
    "                    nmi_sample.append(value)\n",
    "                    nmi_result.append(docID_assign_z[key])\n",
    "            NMI_batch[batch_i] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "            topic_batch[batch_i] = len(Topics)\n",
    "            print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "    if batch_i != 1:\n",
    "        start = int(len(corpus) / total_batch) * (batch_i -1) + 1\n",
    "    time_stop = timeit.default_timer()\n",
    "    time_iteration_list.append(time_stop-time_start)\n",
    "    print(time_iteration_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8700457677533218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "[0.8803076523452149, 0.8687252927079312, 0.8690938337150865, 0.8724089857949292, 0.8682097627878833, 0.8829909798277922, 0.884561511160292, 0.8632691421539366, 0.8562621211408585, 0.8700285470763935, 0.8795298234923449, 0.8683806692576711, 0.8619609115942072, 0.8747436197854798, 0.8670512491432407, 0.856084078731455]\n",
      "0.8702255112946697\n"
     ]
    }
   ],
   "source": [
    "common_keys = []\n",
    "\n",
    "item_NMI = []\n",
    "for key,value in sorted(NMI_batch.items(), key=lambda x: x[0]):\n",
    "    common_keys.append(key)\n",
    "    item_NMI.append(value)\n",
    "print(common_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "print(item_NMI)\n",
    "print(np.mean(item_NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha  0.1  NMI:  0.8553973896584917\n",
      "alpha  0.1  Topics:  109\n",
      "alpha  0.2  NMI:  0.8586395598047273\n",
      "alpha  0.2  Topics:  113\n",
      "alpha  0.3  NMI:  0.8621052092575683\n",
      "alpha  0.3  Topics:  117\n",
      "alpha  0.4  NMI:  0.8617021553300233\n",
      "alpha  0.4  Topics:  121\n",
      "alpha  0.5  NMI:  0.8619820784046283\n",
      "alpha  0.5  Topics:  124\n",
      "alpha  0.6  NMI:  0.8652709179319698\n",
      "alpha  0.6  Topics:  126\n",
      "alpha  0.7  NMI:  0.865502768912514\n",
      "alpha  0.7  Topics:  127\n",
      "alpha  0.8  NMI:  0.8665753557337122\n",
      "alpha  0.8  Topics:  129\n",
      "alpha  0.9  NMI:  0.8666900836555417\n",
      "alpha  0.9  Topics:  132\n",
      "alpha  1.0  NMI:  0.8679872662269225\n",
      "alpha  1.0  Topics:  132\n"
     ]
    }
   ],
   "source": [
    "alpha_NMI = {}\n",
    "alpha_topics = {}\n",
    "for alpha in np.arange(0.1,1.1,0.1):\n",
    "    alpha = np.around(alpha,decimals=1)\n",
    "    \n",
    "    total_batch = 16\n",
    "    NMI_batch = {}\n",
    "    start = 0\n",
    "    end = 0\n",
    "    topic_batch = {}\n",
    "    time_iteration_list = []\n",
    "    for batch_i in range(1,17):\n",
    "        if batch_i == 16:\n",
    "            end = int(len(corpus))\n",
    "        else:\n",
    "            end = int(len(corpus) / total_batch) * batch_i\n",
    "\n",
    "        docID_assign_z = {}\n",
    "        m_z = {}\n",
    "        n_z = {}\n",
    "        n_w = {}\n",
    "        n_b = {}\n",
    "        Topics = []\n",
    "        V = set()\n",
    "        D = set()\n",
    "        #alpha = 0.6\n",
    "        beta = 0.02\n",
    "        NMI_batch[batch_i] = 0\n",
    "        topic_batch[batch_i] = 0\n",
    "\n",
    "        time_start = timeit.default_timer()\n",
    "        total_iter = 10\n",
    "        for iter in range(total_iter):\n",
    "            for docID in range(start,end):\n",
    "                words = corpus[docID]\n",
    "                D.discard(docID)\n",
    "                if docID in docID_assign_z.keys():\n",
    "                    before_k = docID_assign_z[docID]\n",
    "                    m_z[before_k].discard(docID)\n",
    "                    for biterm in words:\n",
    "                        for word in biterm.split(','):\n",
    "                            n_z[before_k][word] -= 1\n",
    "                            n_w[before_k] -=1\n",
    "                else:\n",
    "                    before_k = -1\n",
    "                if len(D) == 0 and len(V) == 0:\n",
    "                    choose_k = 0\n",
    "                    D.add(docID)\n",
    "                    docID_assign_z[docID] = choose_k\n",
    "                    if choose_k not in m_z.keys():\n",
    "                        m_z[choose_k] = set()\n",
    "                    m_z[choose_k].add(docID)\n",
    "                    for biterm in words:\n",
    "                        for word in biterm.split(','):\n",
    "                            if choose_k not in n_w.keys():\n",
    "                                n_w[choose_k] = 0\n",
    "                            if choose_k not in n_z.keys():\n",
    "                                n_z[choose_k] = {}\n",
    "                            if word not in n_z[choose_k].keys():\n",
    "                                n_z[choose_k][word] = 0\n",
    "                            n_z[choose_k][word] += 1\n",
    "                            n_w[choose_k] += 1\n",
    "                            V.add(word)\n",
    "                    if choose_k not in Topics:\n",
    "                        Topics.append(choose_k)\n",
    "                else:\n",
    "                    log_pro = []\n",
    "                    for k in Topics:\n",
    "                        pro_k = len(m_z[k])\n",
    "                        if pro_k != 0:\n",
    "                            i = 0\n",
    "                            for biterm in words:\n",
    "                                each_word = biterm.strip().split(',')\n",
    "                                for word in each_word:\n",
    "                                    if word not in n_z[k].keys():\n",
    "                                        n_z[k][word] = 0\n",
    "                                for j in range(doc_biterm[docID][biterm]):\n",
    "                                    pro_k *= (n_z[k][each_word[0]] + n_z[k][each_word[-1]] + beta + j) / ( n_w[k] + len(V)*beta + i) \n",
    "                                    i += 1\n",
    "\n",
    "                        if pro_k == 0:\n",
    "                            pro_k = sys.float_info.min\n",
    "                        log_pro.append(pro_k)\n",
    "\n",
    "                    pro_new_k = alpha*(len(D))\n",
    "                    i = 0\n",
    "                    for biterm in words:\n",
    "                        for j in range(doc_biterm[docID][biterm]):\n",
    "                            pro_new_k *= ( beta + j) / ( len(V)*beta + i) \n",
    "                            i += 1\n",
    "\n",
    "                    if pro_new_k == 0:\n",
    "                        pro_new_k = sys.float_info.min        \n",
    "                    log_pro.append(pro_new_k)\n",
    "\n",
    "                    sum_pro=sum(log_pro)\n",
    "\n",
    "                    normalized_posterior = [i/sum_pro for i in log_pro]    \n",
    "                    select_k = None\n",
    "                    if iter == (total_iter - 1):\n",
    "                        select_k = normalized_posterior.index(max(normalized_posterior))\n",
    "\n",
    "                    else:\n",
    "                        select_k = np.random.choice( (len(Topics)+1) , 1, p=normalized_posterior)[0]  \n",
    "\n",
    "                    if select_k == len(Topics):\n",
    "                        choose_k = np.max(Topics) + 1\n",
    "                    else:\n",
    "                        choose_k = Topics[select_k]\n",
    "\n",
    "                    D.add(docID)\n",
    "                    docID_assign_z[docID] = choose_k\n",
    "                    if choose_k not in m_z.keys():\n",
    "                        m_z[choose_k] = set()\n",
    "                    m_z[choose_k].add(docID)\n",
    "                    for biterm in words:\n",
    "                        for word in biterm.split(','):\n",
    "                            if choose_k not in n_w.keys():\n",
    "                                n_w[choose_k] = 0\n",
    "                            if choose_k not in n_z.keys():\n",
    "                                n_z[choose_k] = {}\n",
    "                            if word not in n_z[choose_k].keys():\n",
    "                                n_z[choose_k][word] = 0\n",
    "                            n_z[choose_k][word] += 1\n",
    "                            n_w[choose_k] += 1\n",
    "                            V.add(word)\n",
    "                    if choose_k not in Topics:\n",
    "                        Topics.append(choose_k)\n",
    "\n",
    "                count_k = []\n",
    "                for k in Topics:\n",
    "                    if k in m_z.keys() and len(m_z[k]) == 0:\n",
    "                        m_z.pop(k, None)\n",
    "                        n_z.pop(k, None)\n",
    "                        n_w.pop(k, None)\n",
    "                        count_k.append(k)\n",
    "                for k in count_k:\n",
    "                    Topics.remove(k)   \n",
    "            if iter == 9:\n",
    "                from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "                nmi_sample = []\n",
    "                nmi_result = []\n",
    "                for key, value in news_labels.items():\n",
    "                    if key < end and key >= start:\n",
    "                        nmi_sample.append(value)\n",
    "                        nmi_result.append(docID_assign_z[key])\n",
    "                NMI_batch[batch_i] = normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result))\n",
    "                topic_batch[batch_i] = len(Topics)\n",
    "                #print(\"start \",start, \" end \",end, \"topics: \",len(Topics),\"truth: \",len(np.unique(nmi_sample)), \" NMI: \",normalized_mutual_info_score(np.array(nmi_sample), np.array(nmi_result)))\n",
    "        if batch_i != 1:\n",
    "            start = int(len(corpus) / total_batch) * (batch_i -1) + 1\n",
    "    \n",
    "    alpha_NMI[alpha] = np.mean(list(NMI_batch.values()))\n",
    "    alpha_topics[alpha] = int(np.mean(list(topic_batch.values())))\n",
    "    print(\"alpha \",alpha, \" NMI: \", alpha_NMI[alpha])\n",
    "    print(\"alpha \",alpha, \" Topics: \", alpha_topics[alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.8553973896584917, 0.8586395598047273, 0.8621052092575683, 0.8617021553300233, 0.8619820784046283, 0.8652709179319698, 0.865502768912514, 0.8665753557337122, 0.8666900836555417, 0.8679872662269225])\n",
      "dict_values([109, 113, 117, 121, 124, 126, 127, 129, 132, 132])\n"
     ]
    }
   ],
   "source": [
    "print(alpha_NMI.values())\n",
    "print(alpha_topics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
